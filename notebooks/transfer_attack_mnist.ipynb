{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from robustness.attacks.fast_gradient import FastGradient\n",
    "from robustness.attacks.projected_gradient_descent import ProjectedGradientDescent\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_A, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(3*3*64, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 3*3*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CNN_B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_B, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(1024, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train and evaulation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, device, epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.\n",
    "        correct = 0\n",
    "        running_confidences = []\n",
    "\n",
    "        # Loop over each batch from the training set\n",
    "        for batch in train_loader:\n",
    "            # Copy data to device if needed\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Unpack the batch from the loader\n",
    "            inputs, labels = batch\n",
    "\n",
    "            # Zero gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            running_loss += loss.item() * len(inputs)\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            confidence, _ = outputs.softmax(dim=1).max(dim=1)\n",
    "            running_confidences += confidence\n",
    "        \n",
    "        metrics = {'loss': running_loss / len(train_loader.dataset),\n",
    "                   'accuracy': correct / len(train_loader.dataset),\n",
    "                   'average_confidence': (sum(running_confidences) / len(train_loader.dataset)).item()}\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate(model, data_loader, device, attack=None):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    running_loss = 0.\n",
    "    running_confidences = []\n",
    "    \n",
    "    # Loop over each batch from the validation set\n",
    "    for batch in data_loader:\n",
    "        \n",
    "        # Copy data to device if needed\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the batch from the loader\n",
    "        inputs, labels = batch\n",
    "\n",
    "        if attack:\n",
    "            inputs, _ = attack.generate(inputs, labels)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            # Update metrics\n",
    "            running_loss += loss.item() * len(inputs)\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            confidence, _ = outputs.softmax(dim=1).max(dim=1)\n",
    "            running_confidences += confidence\n",
    "\n",
    "    metrics = {'loss': running_loss / len(data_loader.dataset),\n",
    "               'accuracy': correct / len(data_loader.dataset),\n",
    "               'average_confidence': (sum(running_confidences) / len(data_loader.dataset)).item()}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mnist(batch_size):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_a_set, train_b_set = torch.utils.data.random_split(train_set, [30000, 30000])\n",
    "\n",
    "    train_a_loader = torch.utils.data.DataLoader(train_a_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    train_b_loader = torch.utils.data.DataLoader(train_b_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return train_a_loader, train_b_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_loader, train_b_loader, test_loader = prepare_mnist(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = CNN_A().to(device)\n",
    "model_b = CNN_B().to(device)\n",
    "\n",
    "optimizer_a = optim.SGD(model_a.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_b = optim.SGD(model_b.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "train(model_a, train_a_loader, optimizer_a, device, epochs=20)\n",
    "train(model_b, train_b_loader, optimizer_b, device, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_a, \"../models/transfer_attack_a_mnist.pt\")\n",
    "torch.save(model_b, \"../models/transfer_attack_b_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model A:', evaluate(model_a, test_loader, device))\n",
    "print('Model B:', evaluate(model_b, test_loader, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = torch.load(\"../models/transfer_attack_a_mnist.pt\").to(device)\n",
    "model_b = torch.load(\"../models/transfer_attack_b_mnist.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, 0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "accuracy = []\n",
    "average_confidence = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    metrics = evaluate(model_a, test_loader, device, attack=FastGradient(model_b, epsilon=epsilon))\n",
    "\n",
    "    accuracy.append(metrics['accuracy'])\n",
    "    average_confidence.append(metrics['average_confidence'])\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.plot(epsilons, accuracy, \"*-\", label='Accuracy')\n",
    "plt.plot(epsilons, average_confidence, \"*-\", label='Average confidence')\n",
    "plt.yticks(np.arange(0, 1, step=0.1))\n",
    "plt.xticks(np.arange(0, 0.5, step=0.1))\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('robustness')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c86240bf0cc73d06c1704dd43be9ce5d1afaeaa71caf709b6aaa420f3af3e84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
